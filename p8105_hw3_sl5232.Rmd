---
title: "p8105_hw3_sl5232"
author: "Shangsi Lin"
date: "2022/10/12"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)

library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Load Data from p8015 dataset.
```{r load_one}
data("instacart")

instacart = 
  instacart %>% 
  as_tibble(instacart)
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns, with each row resprenting a single product from an instacart order. Variables include identifiers for user, order, and product; the order in which each product was added to the cart. There are several order-level variables, describing the day and time of the order, and number of days since prior order. Then there are several item-specific variables, describing the product name (e.g. Yogurt, Avocado), department (e.g. dairy and eggs, produce), and aisle (e.g. yogurt, fresh fruits), and whether the item has been ordered by this user in the past. In total, there are `r instacart %>% select(product_id) %>% distinct %>% count` products found in `r instacart %>% select(user_id, order_id) %>% distinct %>% count` orders from `r instacart %>% select(user_id) %>% distinct %>% count` distinct users.

```{r answer_aisle}
aisle_df = instacart %>%
  count(aisle, name = "n_orders") %>% 
  arrange(desc(n_orders))
```
There are `r nrow(distinct(instacart, aisle_id))` distinct aisles, and the top five aisles from which most items are ordered from arefresh vegetables, fresh fruits, packaged vegetables fruits, yogurt, and packaged cheese.

```{r plot_item}
aisle_df %>% 
  filter(n_orders > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n_orders)) %>% 
  ggplot(aes(x = aisle, y = n_orders)) + 
  geom_point() + 
  labs(title = "Number of items for each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

The above code chunk makes a plot that shows the number of items ordered in each aisle that has more than 10000 items ordered.Aisles are arranged in a low to high fashion according to the number of items ordered.

```{r popular_table}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

The above code chunk shows the three most popular items and their ordered times in each of the aisles "baking ingredients", "dog food care", "packaged vegetables fruits".

```{r ordered_time}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```

The above code chunk shows the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

## Problem 2

First,load and clean the dataset. Used `pivot_longer` function to modify the dataset to present each individual activity count with their corresponding day id, week, and day of the week. A new parameter called "day_type" is also introduced to indicate whether the particular activity happened in a weekday or weekend. 

```{r load_two}
accel_df = read.csv("data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    cols = activity_1:activity_1440,
    names_to = "activity_number",
    values_to = "activity_counts"
  ) %>% 
  mutate(day_type = ifelse(day == ("Sunday") | day == ("Saturday"), "weekends", "weekdays")) %>% 
  mutate(activity_number = activity_number %>% str_replace("^[a-z, _]*", "")) %>% 
  mutate(activity_number = as.numeric(activity_number)) %>% 
  select(week, day_id, day, day_type, everything())
```

In total, there are  `r nrow(accel_df)` observations and `r ncol(accel_df)` variable classes in the resulting dataset.

Next, make a table to show the total daily activities for each day. Arrange them in decreasing order of the daily activity counts that each day have. 

```{r accel_table}
accel_df %>% 
  group_by(day_id, day, day_type) %>% 
  summarize(daily_counts = sum(activity_counts)) %>% 
  arrange(desc(daily_counts)) %>% 
  knitr::kable()
```

From the above table we can see that among the top 10 days with the most daily activities, only 2 of them are weekends, which implies that activities are less during weekends. For further investigation, a histogram showing the total daily activity counts for each day of the week across the five week recording period is made.

```{r histo_accel}
accel_df %>%
  group_by(day) %>%
  summarize(daily_counts = sum(activity_counts)) %>% 
  ggplot(aes(x = day, y = daily_counts, fill = day)) + 
  geom_histogram(stat = "identity") + 
  labs(
    x = "Day of the Week",
    y = "Total Activity Counts",
    title = "Total Daily Activity Counts Recorded by Accelerometer"
  ) 
```

From the above histogram we can see that Saturday has the least amount of total daily activity while Friday has the most total daily activity. To investigate the trend for each individual day, a scatterplot is made below.

```{r scatter_accel}
accel_df %>%
  group_by(day_id) %>%
  summarize(daily_counts = sum(activity_counts)) %>% 
  ggplot(aes(x = day_id, y = daily_counts)) + 
  geom_point(size = 3) +
  geom_smooth(se = TRUE) +
  labs(
    title = "Daily Activity Counts",
    x = "Day ID",
    y = "Activity Counts"
  )
```

From the scatter plot above, not much useful information can we gain as the trend line moved up and down and eventually up again throughout the recorded period, with relatively large confidence interval. There are very large differences between each individual day so more data is needed for a clear trend.

The code below will generate a 24-hour activity time courses for each day and each color to indicate the day of the week.

```{r plot_minutes}
accel_df %>%
  group_by(day, activity_number) %>%
  mutate(minute_counts = sum(activity_counts)) %>% 
  distinct(day, activity_number, minute_counts) %>%  
  ggplot(aes(x = activity_number, y = minute_counts, color = day)) + 
  geom_point(alpha = 0.1) +
  geom_smooth(se = TRUE, size = 2) +
  labs(
    title = "24-hour Activity Time Courses",
    x = "Minute in Day",
    y = "Activity Counts"
  ) +
  scale_x_continuous(breaks = seq(0, 1440, by = 60))
```

By reading the graph above and focusing on its trendlines, we observe that activities are relatively low in the beginning of the 300 minutes of the day, possibly due to sleep. Then activity counts rises between 300 minutes to 420 minutes of the day, and stays relatively high and steady from 420 minutes to 1260 minutes of the day. In the end of the day from 1260 minutes to 1440 minutes, the activity counts drop substantially. It is worth to note that Sunday and Friday have special peaks of activity counts at around 660 minutes and 1260 minutes, respectively. Thus we can conclude that activity counts are closely linked to the status of the individual wearing the accelerometer, the activity counts are high when he is awake and low when he goes to bed.

## Problem 3

```{r load_three}
library(p8105.datasets)
data("ny_noaa")
```

There are `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns of data in this dataset. The content of this dataset are statistics from weather stations in New York State. Varaibles in this dataset include the weather station ID, date of the weather data, and other weather statistics including precipitation(prcp), snowfall(snow), snow depth(snwd), tmax and tmin. There are a lot of missing data entires in this dataset, so it would be an issue to try to directly visualize this dataset.

```{r clean_three}
ny_noaa = ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(
    prcp = prcp / 10,
    year = as.numeric(year),
    month = as.numeric(month),
    day = as.numeric(day),
    tmax = as.numeric(tmax) / 10,
    tmin = as.numeric(tmin) / 10,
  ) 
```

```{r snow}
ny_noaa %>% 
  count(snow) %>%
  arrange(desc(n))
```

For snowfall, the most commonly observed value is 0, this is because of the fact that New York State don't have snows all year round, most of the times there are no snow at all in a day so the most commonly observed value for snowfall is 0.

Make a two-panel plot showing the average max temperature in January and July in each station across years.

```{r average_tmax}
ny_noaa %>% 
  filter(month ==  c("1","7")) %>% 
  mutate(month = recode(month, "1" = "January", "7" = "July")) %>% 
  drop_na(tmax) %>% 
  select(-prcp, -snow, -snwd, -tmin) %>% 
  group_by(id, year, month) %>% 
  mutate(mean_tmax = mean(tmax)) %>% 
  distinct(id, year, month, mean_tmax) %>% 
  ggplot(aes(x = year, y = mean_tmax, color = month)) + 
  geom_point(alpha = 0.1) +
  geom_smooth(se = TRUE, size = 2) +
  labs(
    title = "Average Maximum Tempreture in January and July Across Years",
    x = "Year",
    y = "Average Maximum Tempreture (°C)"
  ) +
  facet_grid(. ~ month)
```

From the trendlines we observe in the graph, an observable structure we can see is that the average maximum temperature of January have relatively large fluctuations across the years while the average maximum temperature of July have relatively small fluctuations. Also we observe cycle-like behaviors in the average maximum temperature from both months across the years. Both months have outliers in terms of average max temperature. For example, a weather station had 14 celcius as its average maximum temperature in January one year while some of the coldest weather stations had about -12 celcius as its average maximum temperature in January.

Make plot for tmax vs tmin for the full dataset and a plot showing the distribution of snowball values greater than 0 and less than 100 separately by year. Then combine them together to form a two-panel plot.

```{r two_panel}
tmaxmin_plot = ny_noaa %>% 
  ggplot(aes(x = tmax, y = tmin)) +
  geom_hex() +
  labs(
    title = "NYS tmax vs tmin temperatures",
    x = "Maximum Temperature(°C)",
    y = "Minimum temperature(°C)") 
tmaxmin_plot

```

```{r snow_distri}
snow_distri = ny_noaa %>% 
  filter(snow > 0, snow < 100) %>% 
  ggplot(aes(x = as.factor(year), y = snow, fill = year)) +
  geom_violin() +
  labs(
    title = "Distribution of Snowfall greater than 0 and less than 100",
    x = "Year",
    y = "Snowfall(mm)",
    caption = "Data from snowfall value (0~100 mm)"
  ) +
  theme(axis.text.x = element_text(angle = 45),
        legend.position = "none")
```